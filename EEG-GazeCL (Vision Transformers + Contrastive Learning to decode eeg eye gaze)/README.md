EEG-GazeCL (original architecture + original methodology)

Contrastive Spatial Decoding of Gaze from EEG

EEG-GazeCL is a deep learning framework for decoding spatial gaze targets from EEG signals.
It learns spatially consistent embeddings using a Vision-Transformer-style EEG encoder and distance-aware contrastive learning.

Evaluated on the EEGEyeNet dataset, EEG-GazeCL achieves improved spatial accuracy, reduced catastrophic errors, and smoother gaze predictions compared to standard classification baselines.

------------------- Key Ideas

EEG gaze decoding is inherently spatial, not just categorical.

Standard cross-entropy treats all errors equally â€” spatial contrastive learning differentiates nearby vs. far errors.

Nearby gaze targets â†’ nearby embeddings; far targets â†’ well-separated embeddings.

Regularization is applied early in training, then annealed.

------------------Model Overview

Input: EEG trials of shape (128 channels Ã— 500 time samples)

Backbone: EEG Vision Transformer (EEGViT)

Temporal convolution â†’ creates time patches

Spatial convolution â†’ groups channels

Transformer encoder with CLS token

Outputs:

Gaze class (discrete dot)

Latent embedding (used for spatial regularization)

--------Spatial Contrastive Learning

We introduce a distance-aware spatial contrastive loss:
âˆ¥ziâ€‹âˆ’zjâ€‹âˆ¥âˆâˆ¥giâ€‹âˆ’gjâ€‹âˆ¥


Where:

ğ‘§_i = EEG embedding

g_i = 2D gaze position


Key design choices:

Contrastive loss applied only in early epochs (annealing)

Gaussian distance weighting

Combined with standard cross-entropy

 ---------Evaluation Metrics

Beyond standard classification accuracy, we measure:

Median pixel error

Mean pixel error

P(exact): exact dot prediction

P(within 1 step): nearest neighbor

P(within 2 steps): two spatial steps

This gives a faithful evaluation of spatial gaze decoding quality.

---------------Dataset

EEGEyeNet â€“ Position Task (Dot Targets)

Each subject file contains:

EEG: (N, T, C) EEG trials

labels: (N, 3) â†’ [trial_id, x, y]

Multiple subjects are stored as separate .npz files and merged during training.

---------------Results (Single Subject)

After spatial contrastive learning + annealing:

Exact accuracy: ~45% (no paper really has fine grained decoding for the exact spatial coordinates out there, so it's pretty hard w/ eeg data)

Within 1 step: ~54%

Within 2 steps: ~71%

Median pixel error: ~55 px (SOTA)

Errors are mostly local, with rare catastrophic failures.

------------- Project Structure
EEG-GazeCL/
â”œâ”€â”€ data/                # EEGEyeNet subject files
â”œâ”€â”€ models/              # EEGViT and loss implementations
â”œâ”€â”€ notebooks/           # Colab / Jupyter demos
â”œâ”€â”€ training/            # Training scripts
â”œâ”€â”€ evaluation/          # Metrics & plotting scripts
â”œâ”€â”€ utils/               # Data loaders, preprocessing
â””â”€â”€ README.md
