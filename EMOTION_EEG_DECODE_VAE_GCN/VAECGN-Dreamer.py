# -*- coding: utf-8 -*-
"""VAEGNNDREAMER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tKMv_CIetB0tH8T4I9CzRukTHU7M3s4C
"""

from mpl_toolkits.mplot3d import Axes3D
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt # plotting
import numpy as np # linear algebra
import os # accessing directory structure
import pandas as pd# data processing, CSV file I/O (e.g. pd.read_csv)
import scipy.io

from google.colab import drive
drive.mount('/content/drive')

from scipy.io import loadmat

path = '/content/drive/MyDrive/DREAMER.mat'  # adjust if you put it in a subfolder
data = loadmat(path)

data.keys()
data = data['DREAMER']
## This is the dict the inluce which feature you want for the patient

Features = {
    'Age' : 0,
    'Gender': 1,
    'EEG' : 2,
    'ECG' : 3,
    'Valence' : 4,
    'Arousal' : 5,
    'Dominance' : 6
}

def retrieve_data(Sample_index, Feature_name):
    """
    Retrieve specific feature data for a given patient from the dataset.

    Parameters:
        Sample_index (int): Index of the patient (0 to 23).
        Feature_name (str): Name of the feature to retrieve. It should be one of the following:
                            'Age', 'Gender', 'EEG', 'ECG', 'Valence', 'Arousal', 'Dominance'.

    Returns:
        float: The value of the specified feature for the specified patient.

    Raises:
        KeyError: If Feature_name is not one of the supported features.
    """
    return data['Data'][0,0][0][Sample_index][0][0][Features[Feature_name]][0]


signal_types = {
    'BaseLine' :0,
    'Stimulie' :1
}

def retrieve_signals(Sample_index, Feature_type, signal_type):
    """
    Retrieve specific signal data for a given patient from the dataset.

    Parameters:
        Sample_index (int): Index of the patient (0 to 23).
        Feature_type (str): Type of feature ('EEG' or 'ECG').
        signal_type (str): Type of signal to retrieve ('BaseLine' or 'Stimulie').

    Returns:
        float: The value of the specified signal for the specified patient.

    Raises:
        ValueError: If Feature_type is not 'EEG' or 'ECG'.
    """
    if Feature_type not in ('EEG', 'ECG'):
        raise ValueError(f"Invalid Feature_type: {Feature_type}. Feature_type must be 'EEG' or 'ECG'.")

    return data['Data'][0,0][0][Sample_index][0][0][Features[Feature_type]][0][0][signal_types[signal_type]]


EEG_Baseline = []

for i in range(23):
    baseline = retrieve_signals(i,'EEG','BaseLine')
    EEG_Baseline.append(baseline)

len(EEG_Baseline[0])

EEG_Stimuli = []

for i in range(23):
    stimuli = retrieve_signals(i,'EEG','Stimulie')
    EEG_Stimuli.append(stimuli)

len(EEG_Stimuli[0])

big_df = pd.DataFrame()

for i in range(23):  # for each patient
    for j in range(18):  # for each video
        patient_index = [i] * len(EEG_Stimuli[i][j][0])
        video_index = [j] * len(EEG_Stimuli[i][j][0])
        electrodes = []
        for k in range(14):  # for each electrode
            electrode = []
            for l, row in enumerate(EEG_Stimuli[i][j][0]):
                electrode.append(row[k])
            electrodes.append(electrode)

        Dict = {
            'patient_index': patient_index,
            'video_index': video_index,
        }
        columns_list = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7', 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']
        for idx, key in enumerate(columns_list):
            Dict[key] = electrodes[idx]

        df = pd.DataFrame(Dict)
        big_df = pd.concat([big_df, df], ignore_index=True)


big_df.head()

max_time_steps = 0
for subj in range(23):
    for vid in range(18):
        sample_df = big_df[(big_df['patient_index'] == subj) & (big_df['video_index'] == vid)]
        max_time_steps = max(max_time_steps, sample_df.shape[0])


eeg_tensor = np.zeros((23, 18, 14, max_time_steps))


channel_names = ['AF3', 'F7', 'F3', 'FC5', 'T7', 'P7',
                 'O1', 'O2', 'P8', 'T8', 'FC6', 'F4', 'F8', 'AF4']

for subj in range(23):
    for vid in range(18):
        sample_df = big_df[(big_df['patient_index'] == subj) & (big_df['video_index'] == vid)]
        eeg = sample_df[channel_names].to_numpy().T  # [14, T]
        T = eeg.shape[1]
        eeg_tensor[subj, vid, :, :T] = eeg  # zero-padded automatically



eeg_tensor.shape

def retrieve_labels(Sample_index, Feature_name):
    """
    Retrieve all 18 video ratings for a given subject and feature ('Valence', 'Arousal', etc.)
    """
    return np.array(data['Data'][0,0][0][Sample_index][0][0][Features[Feature_name]], dtype=float).flatten()

valence_labels = []
arousal_labels = []

for subj in range(23):
    valences = retrieve_labels(subj, 'Valence')  # shape (18,)
    arousals = retrieve_labels(subj, 'Arousal')  # shape (18,)

    for vid in range(18):
        valence_labels.append(1 if valences[vid] >= 3 else -1)
        arousal_labels.append(1 if arousals[vid] >= 3 else -1)

valence_labels = np.array(valence_labels)  # shape (414,)
arousal_labels = np.array(arousal_labels)  # shape (414,)

valence_labels = np.array(valence_labels)
arousal_labels = np.array(arousal_labels)

valence_labels = (valence_labels + 1) // 2
arousal_labels = (arousal_labels + 1) // 2

eeg_tensor.shape



# Original layout
eeg_topo = [
    [None, None, None, 'FP1',  'FPZ',  'FP2', None, None, None],
    [None, None, 'AF7', 'AF3',  'AFZ',  'AF4', 'AF8', None, None],
    ['F7',  'F5',   'F3',   'F1',  'FZ',  'F2',  'F4',  'F6',  'F8'],
    ['FT7', 'FC5',  'FC3',  'FC1', 'FCZ', 'FC2', 'FC4', 'FC6', 'FT8'],
    ['T7',  'C5',   'C3',   'C1',  'CZ',  'C2',  'C4',  'C6',  'T8'],
    ['TP7', 'CP5',  'CP3',  'CP1', 'CPZ', 'CP2', 'CP4', 'CP6', 'TP8'],
    ['P7',  'P5',   'P3',   'P1',  'PZ',  'P2',  'P4',  'P6',  'P8'],
    [None, None, 'PO7',  'PO3',  'POZ', 'PO4', 'PO8', None, None],
    [None, None, None, 'O1',   'OZ',   'O2',  None,  None,  None],
]



# Your 14 channels
my_channels = ['AF3'	,'F7'	,'F3'	,'FC5',	'T7'	,'P7'	,'O1'	,'O2'	,'P8'	,'T8',	'FC6',	'F4',	'F8',	'AF4']

# Filter the layout to only keep the electrodes in your dataset
filtered_topo = [
    [ch if ch in my_channels else None for ch in row]
    for row in eeg_topo
]
filtered_topo
eeg = eeg_tensor.reshape(23 * 18, 14, 50432)  # (414, 14, T)

import numpy as np

# Create a mapping: channel name → (row, col)
channel_to_pos = {
    ch: (i, j)
    for i, row in enumerate(filtered_topo)
    for j, ch in enumerate(row)
    if ch is not None
}

# Initialize grid template: (14 channels placed into 9x9 matrix)
H, W = 9, 9

X_ST = np.zeros((eeg.shape[0], eeg.shape[2], H, W))  # (414, T, 9, 9)

channel_names = my_channels  # in your order

for ch_idx, ch_name in enumerate(channel_names):
    if ch_name not in channel_to_pos:
        continue  # skip channels not in topo
    i, j = channel_to_pos[ch_name]
    # Assign this channel's time series to the right grid position
    X_ST[:, :, i, j] = eeg[:, ch_idx, :]

valence_labels.shape

arousal_labels.shape

from torch.utils.data import TensorDataset, DataLoader

eeg_data = eeg[:, :, -1280:]

!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-sparse -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-cluster -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-spline-conv -f https://data.pyg.org/whl/torch-2.0.0+cpu.html
!pip install torch-geometric

import torch
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv, GraphConv, GATConv  # Common layers

import numpy as np
from scipy.signal import butter, sosfiltfilt

def bandpass_filter_batch(eeg, low, high, fs, order=4):
    """
    Applies a bandpass filter to all EEG channels at once.

    Args:
        eeg (np.ndarray): EEG data of shape [C, T] (channels × time).
        low (float): Lower cutoff frequency in Hz.
        high (float): Upper cutoff frequency in Hz.
        fs (int): Sampling rate in Hz.
        order (int): Filter order.

    Returns:
        np.ndarray: Filtered EEG of shape [C, T].
    """
    sos = butter(order, [low, high], btype='bandpass', fs=fs, output='sos')
    return sosfiltfilt(sos, eeg, axis=1)

def filter_all_bands(eeg, fs):
    """
    Filters EEG into θ, α, β, γ bands.

    Args:
        eeg (np.ndarray): EEG data of shape [C, T].
        fs (int): Sampling frequency.

    Returns:
        dict[str, np.ndarray]: Dictionary of band names to filtered EEG [C, T].
    """
    bands = {
        'delta': (1, 4),
        'theta': (4, 8),
        'alpha': (8, 13),
        'beta':  (13, 30),
        'gamma': (30, 45)
    }

    return {
        name: bandpass_filter_batch(eeg, low, high, fs)
        for name, (low, high) in bands.items()
    }

def compute_de(eeg_band):
    """
    Computes DE for each channel of a band-filtered EEG.
    Input: eeg_band shape [C, T]
    Output: DE values shape [C]
    """
    var = np.var(eeg_band, axis=1, ddof=1)  # shape: [C]
    var = np.clip(var, 1e-8, None)  # Prevent log(0)
    return 0.5 * np.log(2 * np.pi * np.e * var)

from sklearn.feature_selection import mutual_info_regression

def compute_mi_matrix(eeg_band):
    """
    Compute mutual information between all channel pairs.
    Input: eeg_band shape [C, T]
    Output: MI matrix shape [C, C]
    """
    C, T = eeg_band.shape
    mi_matrix = np.zeros((C, C))
    for i in range(C):
        for j in range(C):
            if i != j:
                mi = mutual_info_regression(eeg_band[i].reshape(-1, 1), eeg_band[j])
                mi_matrix[i, j] = mi[0]
    return mi_matrix

def normalize_adjacency(A):
    A = A.copy()
    np.fill_diagonal(A, 0)
    A /= A.max() + 1e-8
    return A

from torch_geometric.data import Data
from torch_geometric.utils import dense_to_sparse, add_self_loops

def mi_matrix_to_graph(A: np.ndarray, node_features=None):
    A_tensor = torch.tensor(A, dtype=torch.float32)

    # Convert to sparse edge list
    edge_index, edge_attr = dense_to_sparse(A_tensor)

    # Add self-loops with meaningful diagonal (mean or original)
    edge_index, edge_attr = add_self_loops(edge_index, edge_attr,
                                           fill_value='mean')  # or A_tensor.diagonal()

    # Convert node features if needed
    if node_features is not None:
        if isinstance(node_features, np.ndarray):
            node_features = torch.tensor(node_features, dtype=torch.float)

    return Data(x=node_features, edge_index=edge_index, edge_attr=edge_attr)

import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv

class GCNEncoder(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.conv1 = GCNConv(1, hidden_dim)
        self.conv2 = GCNConv(hidden_dim, hidden_dim)

    def forward(self, x, edge_index, edge_attr=None):
        x = F.relu(self.conv1(x, edge_index))
        x = F.relu(self.conv2(x, edge_index))
        return x  # shape: [num_nodes, hidden_dim]

class GCNFeatureExtractor(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.gcn = GCNEncoder(hidden_dim)

    def forward(self, batched_band_graphs: dict):
        """
        Args:
            batched_band_graphs: dict of 5 PyG Batches (one per band), each with:
                x: [B * N, 1]
                edge_index: [2, total_edges]
                edge_attr (optional)
                batch: [B * N]

        Returns:
            Tensor of shape [B, 5, N, D]
        """
        band_features = []

        for band in ['delta', 'theta', 'alpha', 'beta', 'gamma']:
            data = batched_band_graphs[band]  # PyG Batch object
            x = self.gcn(data.x, data.edge_index, data.edge_attr)  # [B * N, D]

            # Group by graph using data.batch: [B, N, D]
            num_graphs = data.num_graphs
            num_nodes = int(x.size(0) // num_graphs)  # assuming all graphs same size
            x = x.view(num_graphs, num_nodes, -1)  # [B, N, D]
            band_features.append(x)

        # Stack across bands → shape: [B, 5, N, D]
        features = torch.stack(band_features, dim=1)
        return features

class EEGGRU(nn.Module):
    def __init__(self, gcn_out_dim, num_nodes, gru_hidden_dim, num_layers=1):
        super(EEGGRU, self).__init__()
        self.num_nodes = num_nodes
        self.gcn_out_dim = gcn_out_dim
        self.gru_input_dim = num_nodes * gcn_out_dim

        self.gru = nn.GRU(
            input_size=self.gru_input_dim,
            hidden_size=gru_hidden_dim,
            num_layers=num_layers,
            batch_first=True
        )

        #self.classifier = nn.Linear(gru_hidden_dim, num_classes)

    def forward(self, band_embeddings):
        """
        band_embeddings: tensor of shape [batch_size, num_bands, num_nodes, gcn_out_dim]
        """
        # Flatten nodes and features into one dimension per time step
        x = band_embeddings.view(band_embeddings.size(0), band_embeddings.size(1), -1)  # shape: [B, T, N*D]

        # Pass through GRU
        output, _ = self.gru(x)  # output: [B, T, H]

        # Take the final hidden state for classification
        final_hidden = output[:, -1, :]  # [B, H]
       # out = self.classifier(final_hidden)  # [B, num_classes]
        return final_hidden

from torch.utils.data import Dataset
import time
class EEGGraphDataset(Dataset):
    def __init__(self, eeg_data_batch, fs):
        """
        eeg_data_batch: np.ndarray of shape [num_samples, C, T]
        fs: sampling rate
        """
        self.eeg_data_batch = eeg_data_batch
        self.fs = fs

    def __len__(self):
        return len(self.eeg_data_batch)

    def __getitem__(self, idx):
        start = time.time()
        eeg = self.eeg_data_batch[idx]  # shape [C, T]

        # 1. Filter EEG into bands
        t1 = time.time()
        filtered_bands = filter_all_bands(eeg, self.fs)
        t2 = time.time()

        # 2. Compute DE features

        de_features = {
            band: compute_de(eeg_band)
            for band, eeg_band in filtered_bands.items()
        }


        node_features_per_band = {
            band: de_features[band].reshape(-1, 1)
            for band in de_features
        }

        # 3. Compute adjacency matrices
        t3 = time.time()
        adjacency_matrices = {
            band: compute_mi_matrix(eeg_band)
            for band, eeg_band in filtered_bands.items()
        }

        # 4. Normalize adjacency
        adjacency_matrices = {
            band: normalize_adjacency(A)
            for band, A in adjacency_matrices.items()
        }

        # 5. Construct graphs
        band_graphs = {
            band: mi_matrix_to_graph(A=adjacency_matrices[band],
                                     node_features=node_features_per_band[band])
            for band in adjacency_matrices
        }

        t4 = time.time()
        print(f"[{idx}] Times (sec): Filter={t2 - t1:.2f}, DE={t3 - t2:.2f}, MI={t4 - t3:.2f}")


        return band_graphs  # dict of 5 PyG Data objects


'''
Your pipeline now produces:

band_graphs = {
  'delta': Data(...),
  'theta': Data(...),
  'alpha': Data(...),
  'beta':  Data(...),
  'gamma': Data(...)
}
'''

from torch_geometric.data import Batch

def collate_graph_dict(samples):
    """
    Collate a list of samples, each a dict of 5 band graphs, into a dict of batched band graphs.

    Args:
        samples: List of dicts, each like:
            {
              'delta': Data(...),
              'theta': Data(...),
              'alpha': Data(...),
              'beta':  Data(...),
              'gamma': Data(...)
            }

    Returns:
        dict[str, Batch]: {
            'delta': Batch(...),
            ...
        }
    """
    batched = {}
    for band in samples[0].keys():
        batched[band] = Batch.from_data_list([sample[band] for sample in samples])
    return batched

import os
import torch
from torch_geometric.data import Data

# Path to your saved graphs
graph_dir = '/content/drive/MyDrive/cached_graphs_2samples/'

# Get all .pt files
graph_files = sorted([f for f in os.listdir(graph_dir) if f.endswith('.pt')])

# Load them safely with weights_only=False
graph_data_list = []
for file in graph_files:
    path = os.path.join(graph_dir, file)
    data = torch.load(path, weights_only=False)  # This will work for full Data objects
    graph_data_list.append(data)

print(f"Loaded {len(graph_data_list)} graph samples.")

from torch.utils.data import DataLoader

gnn_loader = DataLoader(
    graph_data_list,  # list of dicts
    batch_size=10,
    collate_fn=collate_graph_dict,
    shuffle=True,
     drop_last=True
)

from torch.utils.data import Dataset, DataLoader



class EEGDataset(Dataset):
    def __init__(self, eeg_data):
        self.eeg_data = eeg_data            # (N, C, 9, 9, T)
        #self.spatial_masks = spatial_masks  # (N, 1, 9, 9)

    def __len__(self):
        return len(self.eeg_data)

    def __getitem__(self, idx):
        return {
            'eeg': self.eeg_data[idx],      # (C, 9, 9, T)
         #   'mask': self.spatial_masks[idx] # (1, 9, 9)
        }

import torch
import torch.nn as nn
import torch.nn.functional as F

class EEGVAE(nn.Module):
    def __init__(self, latent_dim=128):
        super(EEGVAE, self).__init__()

        # ---- ENCODER ----
        self.encoder = nn.Sequential(
            nn.Conv3d(1, 128, kernel_size=(16, 3, 3), stride=(4, 2, 2), padding=(0, 1, 1)),  # → (128, 317, 5, 5)
            nn.ReLU(),
            nn.Conv3d(128, 256, kernel_size=(8, 3, 3), stride=(4, 2, 2), padding=(0, 1, 1)),  # → (256, 78, 3, 3)
            nn.ReLU(),
            nn.Conv3d(256, 256, kernel_size=(4, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1)),  # → (256, 38, 2, 2)
            nn.ReLU(),
            nn.Conv3d(256, 512, kernel_size=(2, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1)),  # → (512, 19, 1, 1)
            nn.ReLU()
        )

        self.flattened_size = 512 * 19 * 1 * 1  # = 38912
        self.fc_mu = nn.Linear(self.flattened_size, latent_dim)
        self.fc_logvar = nn.Linear(self.flattened_size, latent_dim)

        # ---- DECODER ----
        self.fc_decode = nn.Linear(latent_dim, self.flattened_size)

        self.decoder = nn.Sequential(
            nn.ConvTranspose3d(512, 256, kernel_size=(2, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1)),  # → (256, 38, 3, 3)
            nn.ReLU(),
            nn.ConvTranspose3d(256, 256, kernel_size=(4, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1)),  # → (256, 78, 5, 5)
            nn.ReLU(),
            nn.ConvTranspose3d(256, 128, kernel_size=(8, 3, 3), stride=(4, 2, 2), padding=(0, 1, 1)),  # → (128, 317, 9, 9)
            nn.ReLU(),
            nn.ConvTranspose3d(128, 1, kernel_size=(16, 3, 3), stride=(4, 2, 2), padding=(0, 1, 1)),   # → (1, 1280, 9, 9)
            nn.Sigmoid()  # constrain output to [0,1] if input is normalized
        )

    def encode(self, x):
        x = self.encoder(x)
        x = x.view(x.size(0), -1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std

    def decode(self, z):
        x = self.fc_decode(z)
        x = x.view(-1, 512, 19, 1, 1)
        return self.decoder(x)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar,z

# --- Load your trained VAE model ---
vae = EEGVAE()
#vae.load_state_dict(torch.load("vae_weights.pth"))
vae = vae.float()
vae.eval()

all_eeg = X_ST[:,-1280:,:,:]

all_eeg.shape

all_eeg = all_eeg[:44,:,:,:]

dataset = EEGDataset(all_eeg)

ae_loader = DataLoader(dataset, batch_size=10, shuffle=False, drop_last=True)

gnn_loader

ae_loader

device = torch.device('cpu')

hidden_dim = 128
gcn_feature_extractor = GCNFeatureExtractor(hidden_dim)

num_nodes = 14
gcn_out_dim = 128
gru_hidden_dim = 128
eeg_gru = EEGGRU(gcn_out_dim, num_nodes, gru_hidden_dim, num_layers=1)

# Make sure model is in eval mode
gcn_feature_extractor.eval()

eeg_gru.eval()

class EmotionClassifier(nn.Module):
    def __init__(self, input_dim=256, output_dim=2):
        super().__init__()
        self.fc = nn.Linear(input_dim, output_dim)  # 256 -> 2 (arousal, valence)

    def forward(self, x):
        logits = self.fc(x)  # shape: (B, 2)
        return logits

classifier_head = EmotionClassifier(input_dim=256, output_dim=2)
classifier_head.eval()

all_features = []
all_preds = []
for vae_batch, gru_batch in zip(ae_loader, gnn_loader):
    # Get latent z from VAE
    vae_input = vae_batch['eeg'].unsqueeze(1).float().to(device)  # (B,1,D,H,W)

    with torch.no_grad():
        mu, logvar = vae.encode(vae_input)
        z = vae.reparameterize(mu, logvar)  # (B, latent_dim)
        print(z.shape)


    # Get GRU logits
    gru_batch = {band: g.to(device) for band, g in gru_batch.items()}
    with torch.no_grad():
        spatial_features = gcn_feature_extractor(gru_batch)  # (B,5,14,128)
        logits = eeg_gru(spatial_features)                   # (B,2)
        print(logits.shape)

    # Concatenate latent vector z to logits along feature dim
    # Make sure shapes align: e.g., expand logits if needed
    combined_features = torch.cat([logits, z], dim=1)  # (B, 2 + latent_dim)
    all_features.append(combined_features)

    # Now, pass combined_features through a classifier head or threshold or save
    # For example, simple linear layer (you need to define it):
    # preds = classifier_head(combined_features)
    with torch.no_grad():
      logits = classifier_head(combined_features)
      preds = (torch.sigmoid(logits) >= 0.5).int()
      all_preds.append(preds)

    # Save or process as needed



# Stack all results
all_features = torch.cat(all_features, dim=0)  # (40, 256)
all_preds = torch.cat(all_preds, dim=0)        # (40, 2)

print("Features shape:", all_features.shape)
print("Predictions shape:", all_preds.shape)

# Ensure shapes are (N,)
arousal_all = arousal_labels.view().long()
valence_all = valence_labels.view().long()

# Convert numpy arrays to torch tensors

arousal_labels[:44].shape

arousal_all.shape

arousal_all = torch.from_numpy(arousal_labels[:44]).long().view(-1)
valence_all = torch.from_numpy(valence_labels[:44]).long().view(-1)

arousal_all.shape

# Build label dataset
labels_ds = TensorDataset(arousal_all, valence_all)

# Must match ae_loader and gnn_loader batch_size and ordering
label_loader = DataLoader(
    labels_ds,
    batch_size=ae_loader.batch_size,
    shuffle=False,
    drop_last=True
)

print("Arousal unique:", torch.unique(arousal_all))
print("Valence unique:", torch.unique(valence_all))

'''
print("Labels min/max:", labels_ds.min().item(), labels_ds.max().item())
print("Labels unique:", labels_ds.unique())
'''

ae_len = len(ae_loader.dataset)

ae_len

gnn_len = len(gnn_loader.dataset)

lbl_len = len(label_loader.dataset)

# Quick shape consistency check before training
ae_len = len(ae_loader.dataset)
gnn_len = len(gnn_loader.dataset)
lbl_len = len(label_loader.dataset)

assert ae_len == gnn_len == lbl_len, \
    f"Dataset size mismatch: VAE={ae_len}, GNN={gnn_len}, Labels={lbl_len}"

gnn_len

len_graphs = len(graph_data_list)
len_eeg = len(all_eeg)  # however you store EEG for VAE
len_labels = len(arousal_labels)  # or valence_labels

min_len = min(len_graphs, len_eeg, len_labels)

min_len

graph_data_list = graph_data_list[:min_len]

gnn_loader = DataLoader(
    graph_data_list,  # list of dicts
    batch_size=10,
    collate_fn=collate_graph_dict,
    shuffle=True,
     drop_last=True
)

import torch
import torch.nn as nn
import torch.nn.functional as F

# ---------- 1. Focal Loss ----------
class BinaryFocalLossWithLogits(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean'):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, targets):
        """
        logits: (B, 2) raw model outputs
        targets: (B, 2) in {0,1}
        """
        bce = F.binary_cross_entropy_with_logits(logits, targets.float(), reduction='none')  # (B,2)
        p = torch.sigmoid(logits)
        pt = targets * p + (1 - targets) * (1 - p)  # prob of the true class
        focal_weight = self.alpha * (1 - pt).pow(self.gamma)
        loss = focal_weight * bce
        if self.reduction == 'mean':
            return loss.mean()
        elif self.reduction == 'sum':
            return loss.sum()
        return loss

# ---------- 2. Triplet-Center Loss ----------
class TripletCenterLoss(nn.Module):
    def __init__(self, centers, margin=0.5, reduction='mean'):
        super().__init__()
        self.centers = centers  # nn.Parameter (num_classes, emb_dim)
        self.margin = margin
        self.reduction = reduction

    def forward(self, feats, labels):
        """
        feats: (B, D) embedding vectors from metric_head
        labels: (B,) int class IDs (0..K-1)
        """
        B, D = feats.shape
        K, _ = self.centers.shape

        feats_exp = feats.unsqueeze(1).expand(B, K, D)       # (B, K, D)
        centers_exp = self.centers.unsqueeze(0).expand(B, K, D) # (B, K, D)
        dists = 0.5 * ((feats_exp - centers_exp) ** 2).sum(dim=2)  # (B, K)

        pos = dists.gather(1, labels.view(-1,1)).squeeze(1)  # (B,)

        mask = torch.ones_like(dists, dtype=torch.bool)
        mask.scatter_(1, labels.view(-1,1), False)
        neg = dists.masked_fill(~mask, float('inf')).min(dim=1).values  # (B,)

        losses = torch.clamp(pos + self.margin - neg, min=0.0)

        if self.reduction == 'mean':
            return losses.mean()
        elif self.reduction == 'sum':
            return losses.sum()
        return losses

# ---------- 3. VAE Loss ----------
def vae_loss_fn(recon_x, x, mu, logvar):
    """
    recon_x, x: (B,1,T,H,W) in [0,1]
    mu, logvar: latent distribution params
    """
    '''
    bce = F.binary_cross_entropy(recon_x, x, reduction='mean')
    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    kl = kl / x.size(0)                     # normalize by batch
    if torch.isnan(recon_loss) or torch.isnan(kl):
        print("NaN in VAE:", recon_loss.item(), kl.item())
    return recon_loss + kl, recon_loss, kl
    #return bce + kl, bce, kl
'''
    recon_loss = F.mse_loss(recon_x, x, reduction="mean")
    # clamp logvar for stability
    logvar = torch.clamp(logvar, -10, 10)
    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    return recon_loss + kl_loss, recon_loss, kl_loss

# ---------- 4. Metric Head ----------
class MetricHead(nn.Module):
    def __init__(self, in_dim, emb_dim=64):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_dim, in_dim),
            nn.ReLU(inplace=True),
            nn.Linear(in_dim, emb_dim)
        )

    def forward(self, x):
        return self.net(x)

def train_epoch(ae_loader, gnn_loader, label_loader, epoch=0, log_interval=10):
    vae.train()
    gcn_feature_extractor.train()
    eeg_gru.train()
    classifier_head.train()
    metric_head.train()

    total_loss = total_fl = total_tc = total_vae = 0.0
    n_batches = 0

    for step, (vae_batch, gnn_batch, y_pair) in enumerate(zip(ae_loader, gnn_loader, label_loader)):
        # ----- 1) Labels -----
        y_arousal, y_valence = y_pair
        y_bin = torch.stack([y_arousal, y_valence], dim=1).to(device).long()  # (B,2)
        y_quad = y_bin[:, 0] * 2 + y_bin[:, 1]                                # (B,)

        # ----- 2) VAE forward -----
        #x = vae_batch['eeg'].float().to(device)
        x = vae_batch['eeg'].unsqueeze(1).float().to(device)  # (B,1,D,H,W)
        x = (x - x.mean()) / (x.std() + 1e-6)




        recon, mu, logvar, z = vae(x)
        #  Fix shape mismatch before computing recon loss
        if recon.shape != x.shape:
          min_len = min(recon.size(2), x.size(2))
          min_h   = min(recon.size(3), x.size(3))
          min_w   = min(recon.size(4), x.size(4))
          recon   = recon[:, :, :min_len, :min_h, :min_w]
          x       = x[:, :, :min_len, :min_h, :min_w]


        # ----- 3) Graph model forward -----
        gnn_batch = {band: data.to(device) for band, data in gnn_batch.items()}
        spatial_feats = gcn_feature_extractor(gnn_batch)
        gru_logits = eeg_gru(spatial_feats)  # (B, 2)

        # ----- 4) Combine features -----
        combined = torch.cat([gru_logits, z], dim=1)
        cls_logits = classifier_head(combined)  # (B, 2)
        feats = metric_head(combined)           # (B, emb_dim)
        #probs = torch.sigmoid(cls_logits)
        print("y_bin unique:", torch.unique(y_bin))
        print("y_bin dtype:", y_bin.dtype)
        # ----- 5) Losses -----
        loss_fl = focal_crit(cls_logits, y_bin.float())
        loss_tc = tc_crit(feats, y_quad)
        loss_vae, loss_recon, loss_kl = vae_loss_fn(recon, x, mu, logvar)

        if torch.isnan(loss_fl):
          print("NaN in Focal Loss!")
        if torch.isnan(loss_tc):
          print("NaN in Triplet-Center Loss!")
        if torch.isnan(loss_vae):
          print("NaN in VAE Loss!")

        loss = beta_fl * loss_fl + beta_tc * loss_tc + beta_vae * loss_vae

        # ----- 6) Backprop -----
        optim.zero_grad(set_to_none=True)
        loss.backward()
        for name, p in vae.named_parameters():
          if torch.isnan(p.grad).any():
            print("NaN gradient in", name)
            break
        for p in vae.parameters():
          if p.grad is not None:
            torch.nan_to_num_(p.grad, nan=0.0, posinf=1.0, neginf=-1.0)


        torch.nn.utils.clip_grad_norm_(
            list(vae.parameters()) +
            list(gcn_feature_extractor.parameters()) +
            list(eeg_gru.parameters()) +
            list(classifier_head.parameters()) +
            list(metric_head.parameters()) +
            [centers], max_norm=5.0
        )
        optim.step()

        # ----- 7) Stats -----
        n_batches += 1
        total_loss += loss.item()
        total_fl   += loss_fl.item()
        total_tc   += loss_tc.item()
        total_vae  += loss_vae.item()

        if (step + 1) % log_interval == 0:
            print(f"[Ep {epoch} | Step {step+1}] "
                  f"Total={total_loss/n_batches:.4f} "
                  f"FL={total_fl/n_batches:.4f} "
                  f"TC={total_tc/n_batches:.4f} "
                  f"VAE={total_vae/n_batches:.4f} "
                  f"(Recon={loss_recon.item():.4f}, KL={loss_kl.item():.4f})")

    return {
        "loss": total_loss / n_batches,
        "focal": total_fl / n_batches,
        "tc": total_tc / n_batches,
        "vae": total_vae / n_batches,
    }

beta_fl, beta_tc, beta_vae = 1.0, 1.0, 1.0
focal_crit = FocalLoss()
num_classes_quad = 4
emb_dim = 64
centers = nn.Parameter(torch.randn(num_classes_quad, emb_dim, device=device))
tc_crit = TripletCenterLoss(centers=centers, margin=0.5)
#tc_crit = triplet_center_loss(centers=centers, margin=0.5)

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction="mean"):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, targets):
        # logits: (B, 2), targets: (B, 2) with {0,1}
        bce_loss = F.binary_cross_entropy_with_logits(
            logits, targets.float(), reduction="none"
        )
        bce_loss = bce_loss.clamp(max=100)  # prevent exploding loss
        probs = torch.sigmoid(logits)          # (B, 2)
        #ce_loss = F.binary_cross_entropy(probs, targets.float(), reduction="none")
        eps = 1e-6
        pt = torch.where(targets == 1, probs, 1 - probs)
        pt = pt.clamp(min=eps, max=1. - eps)   # prevents log(0) and 0^gamma
        pt = torch.where(targets == 1, probs, 1 - probs)
        focal = self.alpha * (1 - pt) ** self.gamma * bce_loss
        return focal.mean() if self.reduction == "mean" else focal.sum()

class TripletCenterLoss(nn.Module):
    def __init__(self, centers, margin=0.5):
        super().__init__()
        self.centers = centers   # nn.Parameter [num_classes, emb_dim]
        self.margin = margin

    def forward(self, feats, labels):
        # feats: (B, emb_dim), labels: (B,)
        dists = torch.cdist(feats, self.centers, p=2) ** 2   # (B, num_classes)

        # distance to correct class
        correct_dists = dists.gather(1, labels.view(-1, 1)).squeeze(1)  # (B,)

        # min distance to any wrong class
        mask = torch.zeros_like(dists).scatter_(1, labels.view(-1, 1), 1.0)
        wrong_dists = dists + 1e5 * mask  # mask out correct class
        min_wrong, _ = wrong_dists.min(dim=1)  # (B,)

        loss = F.relu(correct_dists + self.margin - min_wrong)
        return loss.mean()

# ---------------------------
# 3. VAE loss
# ---------------------------
def vae_loss_fn(recon_x, x, mu, logvar):
    recon_loss = F.mse_loss(recon_x, x, reduction="mean")
    logvar = torch.clamp(logvar, -10, 10)   # ✅ clamp for stability
    #kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    #return recon_loss + kl_loss, recon_loss, kl_loss
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    kl = kl / x.size(0)                     # normalize by batch
    if torch.isnan(recon_loss) or torch.isnan(kl):
        print("NaN in VAE:", recon_loss.item(), kl.item())
    return recon_loss + kl, recon_loss, kl

# ---------------------------
# 4. Training loop
# ---------------------------

'''
def train_epoch(ae_loader, gnn_loader, label_loader, epoch=0, log_interval=10):
    vae.train()
    gcn_feature_extractor.train()
    eeg_gru.train()
    classifier_head.train()
    metric_head.train()

    total_loss = total_fl = total_tc = total_vae = 0.0
    n_batches = 0

    for step, (vae_batch, gnn_batch, y_pair) in enumerate(zip(ae_loader, gnn_loader, label_loader)):

        # ----- 1) Labels -----
        y_arousal, y_valence = y_pair
        y_bin = torch.stack([y_arousal, y_valence], dim=1).to(device).long()  # (B,2)
        y_quad = y_bin[:, 0] * 2 + y_bin[:, 1]                                # (B,)

        # ----- 2) VAE forward -----
        x = vae_batch['eeg'].float().to(device)


        x = vae_batch['eeg'].float().to(device)
        # If shape is (B, 1280, 9, 9), add channel before depth
        if x.ndim == 4:  # (B, D, H, W)
          vae_input = x.unsqueeze(1)         # (B, 1, D, H, W)
        # If shape is (B, batch, D, H, W) because DataLoader wrapped weirdly
        elif x.ndim == 5 and x.shape[1] != 1:  # (B, something, D, H, W)
          vae_input = x.permute(1,0,2,3,4)   # re-order so batch is first
          vae_input = vae_input[:,None,...]  # (B, 1, D, H, W)




        recon, mu, logvar, z = vae(x)

        # ----- 3) Graph model forward -----
        gnn_batch = {band: data.to(device) for band, data in gnn_batch.items()}
        spatial_feats = gcn_feature_extractor(gnn_batch)
        gru_logits = eeg_gru(spatial_feats)  # (B, 2)

        # ----- 4) Combine features -----
        combined = torch.cat([gru_logits, z], dim=1)
        cls_logits = classifier_head(combined)  # (B, 2)
        feats = metric_head(combined)           # (B, emb_dim)

        # ----- 5) Losses -----
        loss_fl = focal_crit(cls_logits, y_bin)
        loss_tc = tc_crit(feats, y_quad)
        loss_vae, loss_recon, loss_kl = vae_loss_fn(recon, x, mu, logvar)

        loss = beta_fl * loss_fl + beta_tc * loss_tc + beta_vae * loss_vae

        # ----- 6) Backprop -----
        optim.zero_grad(set_to_none=True)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(
            list(vae.parameters()) +
            list(gcn_feature_extractor.parameters()) +
            list(eeg_gru.parameters()) +
            list(classifier_head.parameters()) +
            list(metric_head.parameters()) +
            [centers], max_norm=5.0
        )
        optim.step()

        # ----- 7) Stats -----
        n_batches += 1
        total_loss += loss.item()
        total_fl   += loss_fl.item()
        total_tc   += loss_tc.item()
        total_vae  += loss_vae.item()

        if (step + 1) % log_interval == 0:
            print(f"[Ep {epoch} | Step {step+1}] "
                  f"Total={total_loss/n_batches:.4f} "
                  f"FL={total_fl/n_batches:.4f} "
                  f"TC={total_tc/n_batches:.4f} "
                  f"VAE={total_vae/n_batches:.4f} "
                  f"(Recon={loss_recon.item():.4f}, KL={loss_kl.item():.4f})")

    return {
        "loss": total_loss / n_batches,
        "focal": total_fl / n_batches,
        "tc": total_tc / n_batches,
        "vae": total_vae / n_batches,
    }

    '''

optim = torch.optim.Adam(
    list(vae.parameters()) +
    list(gcn_feature_extractor.parameters()) +
    list(eeg_gru.parameters()) +
    list(classifier_head.parameters()) +
    list(metric_head.parameters()) +
    [centers], lr=1e-3
)



# Project combined features into embedding space for triplet-center loss
class MetricHead(nn.Module):
    def __init__(self, input_dim=256, emb_dim=64):
        super().__init__()
        self.fc = nn.Linear(input_dim, emb_dim)

    def forward(self, x):
        return self.fc(x)

metric_head = MetricHead(input_dim=256, emb_dim=64).to(device)

optim = torch.optim.Adam(
    list(vae.parameters()) +
    list(gcn_feature_extractor.parameters()) +
    list(eeg_gru.parameters()) +
    list(classifier_head.parameters()) +
    list(metric_head.parameters()) +   # <-- now exists
    [centers],
    lr=1e-5
)

for epoch in range(1):
    stats = train_epoch(ae_loader, gnn_loader, label_loader, epoch)
    print(stats)





import torch
import torch.nn as nn
import torch.nn.functional as F

class EEGVAE(nn.Module):
    def __init__(self, latent_dim=128):
        super(EEGVAE, self).__init__()

        # ---- ENCODER ----
        self.encoder = nn.Sequential(
            nn.Conv3d(1, 128, kernel_size=(16, 3, 3), stride=(4, 2, 2), padding=(0, 1, 1)),  # → (128, 317, 5, 5)
            nn.ReLU(),
            nn.Conv3d(128, 256, kernel_size=(8, 3, 3), stride=(4, 2, 2), padding=(0, 1, 1)),  # → (256, 78, 3, 3)
            nn.ReLU(),
            nn.Conv3d(256, 256, kernel_size=(4, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1)),  # → (256, 38, 2, 2)
            nn.ReLU(),
            nn.Conv3d(256, 512, kernel_size=(2, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1)),  # → (512, 19, 1, 1)
            nn.ReLU()
        )

        self.flattened_size = 512 * 19 * 1 * 1  # = 38912
        self.fc_mu = nn.Linear(self.flattened_size, latent_dim)
        self.fc_logvar = nn.Linear(self.flattened_size, latent_dim)

        # ---- DECODER ----
        self.fc_decode = nn.Linear(latent_dim, self.flattened_size)

        self.decoder = nn.Sequential(
            nn.ConvTranspose3d(512, 256, kernel_size=(2, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1)),  # → (256, 38, 3, 3)
            nn.ReLU(),
            nn.ConvTranspose3d(256, 256, kernel_size=(4, 3, 3), stride=(2, 2, 2), padding=(0, 1, 1)),  # → (256, 78, 5, 5)
            nn.ReLU(),
            nn.ConvTranspose3d(256, 128, kernel_size=(8, 3, 3), stride=(4, 2, 2), padding=(0, 1, 1)),  # → (128, 317, 9, 9)
            nn.ReLU(),
            nn.ConvTranspose3d(128, 1, kernel_size=(16, 3, 3), stride=(4, 2, 2), padding=(0, 1, 1)),   # → (1, 1280, 9, 9)
            nn.Sigmoid()  # constrain output to [0,1] if input is normalized
        )

    def encode(self, x):
        x = self.encoder(x)
        x = x.view(x.size(0), -1)
        mu = self.fc_mu(x)
        logvar = self.fc_logvar(x)
        return mu, logvar

    def reparameterize(self, mu, logvar):
      logvar = torch.clamp(logvar, -10, 10)   # stop exp overflow
      std = torch.exp(0.5 * logvar)
      eps = torch.randn_like(std)
      return mu + eps * std


    def decode(self, z):
        x = self.fc_decode(z)
        x = x.view(-1, 512, 19, 1, 1)
        return self.decoder(x)

    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar,z

class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction="mean"):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, targets):
        # logits: (B, 2), targets: (B, 2) with {0,1}
        bce_loss = F.binary_cross_entropy_with_logits(
            logits, targets.float(), reduction="none"
        )
        bce_loss = bce_loss.clamp(max=100)  # prevent exploding loss

        probs = torch.sigmoid(logits)  # (B, 2)
        eps = 1e-6
        #  Only keep clamped pt
        pt = torch.where(targets == 1, probs, 1 - probs)
        pt = pt.clamp(min=eps, max=1. - eps)

        focal = self.alpha * (1 - pt) ** self.gamma * bce_loss
        return focal.mean() if self.reduction == "mean" else focal.sum()

def triplet_center_loss(embeddings, labels, centers, margin=1.0):
    # embeddings: (B, D)
    # labels: (B,)
    # centers: (num_classes, D)

    # distances to centers
    dists = torch.cdist(embeddings, centers, p=2)  # (B, num_classes)
    dists = dists.clamp(min=1e-6, max=1e6)        #  prevents NaNs

    # get positive and negative distances
    pos_dists = dists.gather(1, labels.unsqueeze(1))  # (B,1)
    neg_dists, _ = dists.min(dim=1, keepdim=True)     # (B,1)
    loss = torch.relu(pos_dists - neg_dists + margin)
    return loss.mean()

def train_epoch(ae_loader, gnn_loader, label_loader, epoch=0, log_interval=10):
    vae.train()
    gcn_feature_extractor.train()
    eeg_gru.train()
    classifier_head.train()
    metric_head.train()

    total_loss = total_fl = total_tc = total_vae = 0.0
    n_batches = 0

    for step, (vae_batch, gnn_batch, y_pair) in enumerate(zip(ae_loader, gnn_loader, label_loader)):
        # Labels 
        y_arousal, y_valence = y_pair
        y_bin = torch.stack([y_arousal, y_valence], dim=1).to(device).long()  # (B,2)
        y_quad = y_bin[:, 0] * 2 + y_bin[:, 1]                                # (B,)

        #  VAE forward
        x = vae_batch['eeg'].unsqueeze(1).float().to(device)  # (B,1,D,H,W)
        recon, mu, logvar, z = vae(x)

        # align shapes for recon loss
        if recon.shape != x.shape:
            min_len = min(recon.size(2), x.size(2))
            min_h   = min(recon.size(3), x.size(3))
            min_w   = min(recon.size(4), x.size(4))
            recon = recon[:, :, :min_len, :min_h, :min_w]
            x     = x[:, :, :min_len, :min_h, :min_w]

        # Graph model forward 
        gnn_batch = {band: data.to(device) for band, data in gnn_batch.items()}
        spatial_feats = gcn_feature_extractor(gnn_batch)
        gru_logits = eeg_gru(spatial_feats)  # (B, 2)

        # Combine features 
        combined = torch.cat([gru_logits, z], dim=1)
        cls_logits = classifier_head(combined)  # (B, 2)
        feats = metric_head(combined)           # (B, emb_dim)

        # Losses 
        probs = torch.sigmoid(cls_logits).clamp(min=1e-6, max=1-1e-6)  # stabilize
        loss_fl = focal_crit(cls_logits, y_bin)  # already uses logits safely
        loss_tc = tc_crit(feats, y_quad)
        loss_vae, loss_recon, loss_kl = vae_loss_fn(recon, x, mu, logvar)

        # total loss
        loss = beta_fl * loss_fl + beta_tc * loss_tc + beta_vae * loss_vae

        #  Backprop 
        optim.zero_grad(set_to_none=True)
        loss.backward()

        # NaN guard on gradients
        for m in [vae, gcn_feature_extractor, eeg_gru, classifier_head, metric_head]:
            for p in m.parameters():
                if p.grad is not None:
                    torch.nan_to_num_(p.grad, nan=0.0, posinf=1.0, neginf=-1.0)

        # gradient clipping (tighter)
        torch.nn.utils.clip_grad_norm_(
            list(vae.parameters()) +
            list(gcn_feature_extractor.parameters()) +
            list(eeg_gru.parameters()) +
            list(classifier_head.parameters()) +
            list(metric_head.parameters()) +
            [centers], max_norm=1.0
        )
        optim.step()

        #Debugging: check for NaNs / ranges
        if torch.isnan(loss):
            print("NaN detected!")
        print(f"logits range: {cls_logits.min().item():.4f} .. {cls_logits.max().item():.4f}")
        print(f"probs range: {probs.min().item():.4f} .. {probs.max().item():.4f}")
        print(f"embeddings range: {feats.min().item():.4f} .. {feats.max().item():.4f}")

        # Stats
        n_batches += 1
        total_loss += loss.item()
        total_fl   += loss_fl.item()
        total_tc   += loss_tc.item()
        total_vae  += loss_vae.item()

        if (step + 1) % log_interval == 0:
            print(f"[Ep {epoch} | Step {step+1}] "
                  f"Total={total_loss/n_batches:.4f} "
                  f"FL={total_fl/n_batches:.4f} "
                  f"TC={total_tc/n_batches:.4f} "
                  f"VAE={total_vae/n_batches:.4f} "
                  f"(Recon={loss_recon.item():.4f}, KL={loss_kl.item():.4f})")

    return {
        "loss": total_loss / n_batches,
        "focal": total_fl / n_batches,
        "tc": total_tc / n_batches,
        "vae": total_vae / n_batches,
    }

import torch
import torch.nn as nn
import torch.nn.functional as F

#  Focal Loss (binary, with logits) 
class FocalLoss(nn.Module):
    def __init__(self, alpha=0.25, gamma=2.0, reduction="mean"):
        super().__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction

    def forward(self, logits, targets):
        """
        logits: (B,2), raw outputs
        targets: (B,2) one-hot {0,1}
        """
        # BCE with logits (stable)
        bce_loss = F.binary_cross_entropy_with_logits(
            logits, targets.float(), reduction="none"
        ).clamp(max=100)

        probs = torch.sigmoid(logits).clamp(min=1e-6, max=1-1e-6)
        pt = torch.where(targets == 1, probs, 1 - probs)
        pt = pt.clamp(min=1e-6, max=1-1e-6)

        focal = self.alpha * (1 - pt) ** self.gamma * bce_loss
        return focal.mean() if self.reduction == "mean" else focal.sum()


#  Triplet-Center Loss 
class TripletCenterLoss(nn.Module):
    def __init__(self, num_classes, feat_dim, margin=1.0):
        super().__init__()
        self.margin = margin
        self.centers = nn.Parameter(torch.randn(num_classes, feat_dim))

    def forward(self, features, labels):
        """
        features: (B, feat_dim)
        labels:   (B,)
        """
        # distances to all centers
        dists = torch.cdist(features, self.centers, p=2).clamp(min=1e-6, max=1e6)  # (B, num_classes)

        # positive and negative distances
        mask = F.one_hot(labels, num_classes=self.centers.size(0)).bool()
        pos_d = (dists * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)  # (B,)
        neg_d = (dists + 1e6 * mask.float()).min(dim=1)[0]                 # (B,)

        loss = F.relu(pos_d - neg_d + self.margin)
        return loss.mean()


#  VAE Loss
def vae_loss_fn(recon_x, x, mu, logvar):
    recon_loss = F.mse_loss(recon_x, x, reduction="mean").clamp(max=1e6)
    kl_loss = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())
    total_loss = recon_loss + kl_loss
    return total_loss, recon_loss, kl_loss

# Suppose embedding dim = 128, 4 quadrant classes
focal_crit = FocalLoss(alpha=0.25, gamma=2.0, reduction="mean").to(device)
tc_crit = TripletCenterLoss(num_classes=4, feat_dim=128, margin=1.0).to(device)



# ⃣ Hyperparameters & Loss Setup


latent_dim = 128
hidden_dim = 128
gru_hidden_dim = 128
metric_emb_dim = 64

beta_fl = 1.0   # weight for Focal Loss
beta_tc = 1.0   # weight for Triplet-Center Loss
beta_vae = 1.0  # weight for VAE loss

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print("Using device:", device)

# --- Losses ---
focal_crit = BinaryFocalLossWithLogits(alpha=0.25, gamma=2.0)
centers = nn.Parameter(torch.randn(4, metric_emb_dim))  # 4 quadrants: 2x2 for arousal/valence
tc_crit = TripletCenterLoss(centers, margin=0.5)

# --- Metric head ---
metric_head = MetricHead(in_dim=gru_hidden_dim + latent_dim, emb_dim=metric_emb_dim)

# --- Classifier ---
classifier_head = EmotionClassifier(input_dim=gru_hidden_dim + latent_dim, output_dim=2)

# --- Move models to device ---
vae = vae.to(device)
gcn_feature_extractor = gcn_feature_extractor.to(device)
eeg_gru = eeg_gru.to(device)
classifier_head = classifier_head.to(device)
metric_head = metric_head.to(device)
centers = centers.to(device)

# --- Optimizer ---
optim = torch.optim.Adam(
    list(vae.parameters()) +
    list(gcn_feature_extractor.parameters()) +
    list(eeg_gru.parameters()) +
    list(classifier_head.parameters()) +
    list(metric_head.parameters()) +
    [centers],
    lr=1e-3
)

# Training Loop


num_epochs = 10
log_interval = 1

for epoch in range(num_epochs):
    stats = train_epoch(ae_loader, gnn_loader, label_loader, epoch, log_interval)
    print(f"Epoch {epoch} Summary: Total={stats['loss']:.4f}, "
          f"Focal={stats['focal']:.4f}, TC={stats['tc']:.4f}, VAE={stats['vae']:.4f}")
    
    # Optionally: save checkpoint
    torch.save({
        'vae': vae.state_dict(),
        'gcn': gcn_feature_extractor.state_dict(),
        'gru': eeg_gru.state_dict(),
        'classifier': classifier_head.state_dict(),
        'metric_head': metric_head.state_dict(),
        'centers': centers.detach(),
        'optim': optim.state_dict()
    }, f'/content/drive/MyDrive/vae_gnn_epoch{epoch}.pt')


# Evaluation 


vae.eval()
gcn_feature_extractor.eval()
eeg_gru.eval()
classifier_head.eval()
metric_head.eval()

all_preds = []
all_labels = []

for vae_batch, gnn_batch, y_pair in zip(ae_loader, gnn_loader, label_loader):
    vae_input = vae_batch['eeg'].unsqueeze(1).float().to(device)
    with torch.no_grad():
        _, _, _, z = vae(vae_input)

    gnn_batch = {band: g.to(device) for band, g in gnn_batch.items()}
    with torch.no_grad():
        spatial_feats = gcn_feature_extractor(gnn_batch)
        gru_logits = eeg_gru(spatial_feats)
        combined_features = torch.cat([gru_logits, z], dim=1)
        logits = classifier_head(combined_features)
        preds = (torch.sigmoid(logits) >= 0.5).int()

    all_preds.append(preds.cpu())
    y_arousal, y_valence = y_pair
    all_labels.append(torch.stack([y_arousal, y_valence], dim=1))

all_preds = torch.cat(all_preds, dim=0)
all_labels = torch.cat(all_labels, dim=0)

# Accuracy per dimension
acc_arousal = (all_preds[:, 0] == all_labels[:, 0]).float().mean()
acc_valence = (all_preds[:, 1] == all_labels[:, 1]).float().mean()

print(f"Evaluation Accuracy -> Arousal: {acc_arousal:.4f}, Valence: {acc_valence:.4f}")




#  Inference Pipeline Function


def predict_eeg_trial(vae, gcn, gru, classifier, metric_head, trial, device=device):
    """
    Input:
        - trial: dict or tensor of EEG data for a single trial
                 e.g. trial['eeg'] -> [channels, time]
                       trial['gnn'] -> dict of band graphs
    Output:
        - predicted arousal and valence (0 or 1)
        - optional intermediate features for inspection
    """
    vae.eval()
    gcn.eval()
    gru.eval()
    classifier.eval()
    metric_head.eval()
    
    # Move to device
    eeg_input = trial['eeg'].unsqueeze(0).unsqueeze(0).float().to(device)  # [1, 1, channels, time]
    
    # VAE forward
    with torch.no_grad():
        x_recon, mu, logvar, z = vae(eeg_input)
    
    # GCN forward
    gnn_batch = {band: g.unsqueeze(0).to(device) for band, g in trial['gnn'].items()}
    with torch.no_grad():
        spatial_feats = gcn(gnn_batch)  # [1, gru_input_dim]
    
    # GRU forward
    with torch.no_grad():
        gru_out = gru(spatial_feats)  # [1, gru_hidden_dim]
    
    # Combine features
    combined_features = torch.cat([gru_out, z], dim=1)
    
    # Classifier
    with torch.no_grad():
        logits = classifier(combined_features)
        probs = torch.sigmoid(logits)
        preds = (probs >= 0.5).int()
    
    # Metric embedding (optional)
    metric_embedding = metric_head(combined_features)
    
    return {
        'pred_arousal': preds[0,0].item(),
        'pred_valence': preds[0,1].item(),
        'probs': probs[0].cpu().numpy(),
        'z_latent': z.cpu().numpy(),
        'gru_out': gru_out.cpu().numpy(),
        'metric_embedding': metric_embedding.cpu().numpy(),
        'vae_mu': mu.cpu().numpy(),
        'vae_logvar': logvar.cpu().numpy(),
        'vae_recon': x_recon.cpu().numpy()
    }

# 
# Quick Batch Testing & Verification


def test_model_batch(vae, gcn, gru, classifier, metric_head, loader, num_samples=5):
    print("=== Quick Batch Test ===")
    for i, (vae_batch, gnn_batch, label_pair) in enumerate(loader):
        if i >= num_samples:
            break
        trial = {
            'eeg': vae_batch['eeg'][0],
            'gnn': {band: gnn_batch[band][0] for band in gnn_batch}
        }
        out = predict_eeg_trial(vae, gcn, gru, classifier, metric_head, trial)
        print(f"Sample {i}:")
        print("  Predicted -> Arousal:", out['pred_arousal'], 
              "Valence:", out['pred_valence'])
        print("  Probabilities:", out['probs'])
        print("  Latent z shape:", out['z_latent'].shape)
        print("  GRU output shape:", out['gru_out'].shape)
        print("  Metric embedding shape:", out['metric_embedding'].shape)
        print("  VAE recon shape:", out['vae_recon'].shape)
        print("-"*50)


# Inspecting Internal States


def inspect_model_layers(trial, vae, gcn, gru, classifier):
    """

    """
    print("Inspection of model internals ")
    
    eeg_input = trial['eeg'].unsqueeze(0).unsqueeze(0).float().to(device)
    print("EEG input shape:", eeg_input.shape)
    
    # VAE forward
    x_recon, mu, logvar, z = vae(eeg_input)
    print("VAE mu shape:", mu.shape, "logvar shape:", logvar.shape, "z shape:", z.shape)
    
    # GCN forward
    gnn_batch = {band: g.unsqueeze(0).to(device) for band, g in trial['gnn'].items()}
    spatial_feats = gcn(gnn_batch)
    print("GCN output (spatial features) shape:", spatial_feats.shape)
    
    # GRU forward
    gru_out = gru(spatial_feats)
    print("GRU output shape:", gru_out.shape)
    
    # Classifier forward
    combined_features = torch.cat([gru_out, z], dim=1)
    logits = classifier(combined_features)
    print("Classifier logits:", logits.cpu().numpy())

    # Optional: visualize latent or GRU features
    import matplotlib.pyplot as plt
    plt.figure(figsize=(6,3))
    plt.plot(z.cpu().numpy()[0], label='VAE Latent z')
    plt.title("Latent Representation (z)")
    plt.legend()
    plt.show()


#  Run a quick check


# Pick first sample from your loader
first_vae_batch, first_gnn_batch, first_label_pair = next(iter(ae_loader))
trial_example = {
    'eeg': first_vae_batch['eeg'][0],
    'gnn': {band: first_gnn_batch[band][0] for band in first_gnn_batch}
}

inspect_model_layers(trial_example, vae, gcn_feature_extractor, eeg_gru, classifier_head)
test_model_batch(vae, gcn_feature_extractor, eeg_gru, classifier_head, metric_head, ae_loader, num_samples=3)







